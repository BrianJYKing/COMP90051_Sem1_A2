{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1620,
   "id": "b6104f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment and list data files\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3653eb",
   "metadata": {},
   "source": [
    "#### Load domain 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "id": "b9457150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 shape: (1000, 3)\n",
      "Label distribution in domain1:\n",
      " label\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6, 22, 34, 76, 501, 977, 1, 2514, 13623, 76, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[222, 31, 4108, 104, 132, 361, 39, 2305, 12, 9...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[736, 7194, 113, 12, 366, 2870, 123, 101, 12, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[48, 1, 2025, 69, 361, 533, 327, 237, 4150, 13...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2973, 66, 1, 1493, 260, 2740, 50, 1027, 50, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  id\n",
       "0  [6, 22, 34, 76, 501, 977, 1, 2514, 13623, 76, ...      0   0\n",
       "1  [222, 31, 4108, 104, 132, 361, 39, 2305, 12, 9...      0   1\n",
       "2  [736, 7194, 113, 12, 366, 2870, 123, 101, 12, ...      0   2\n",
       "3  [48, 1, 2025, 69, 361, 533, 327, 237, 4150, 13...      0   3\n",
       "4  [2973, 66, 1, 1493, 260, 2740, 50, 1027, 50, 1...      0   4"
      ]
     },
     "execution_count": 1621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load domain1 JSON as newline-delimited JSON (JSON Lines)\n",
    "df1 = pd.read_json('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/domain1_train_data.json', lines=True)\n",
    "print(\"Domain1 shape:\", df1.shape)\n",
    "print(\"Label distribution in domain1:\\n\", df1['label'].value_counts())\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7473ee0",
   "metadata": {},
   "source": [
    "#### Load domain 2 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "id": "9a2bbd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 shape: (5000, 3)\n",
      "Label distribution in domain1:\n",
      " label\n",
      "1    4750\n",
      "0     250\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[22, 6065, 76, 119, 13027, 575, 219, 22, 2435,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1275, 1509, 12, 6113, 6287, 327, 411, 1139, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[575, 2962, 529, 4624, 39, 279, 1012, 277, 76,...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12, 6113, 2428, 69, 375, 1025, 2605, 76, 101,...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[529, 76, 1509, 861, 1, 645, 1, 5013, 237, 3, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  id\n",
       "0  [22, 6065, 76, 119, 13027, 575, 219, 22, 2435,...      0   0\n",
       "1  [1275, 1509, 12, 6113, 6287, 327, 411, 1139, 2...      0   1\n",
       "2  [575, 2962, 529, 4624, 39, 279, 1012, 277, 76,...      0   2\n",
       "3  [12, 6113, 2428, 69, 375, 1025, 2605, 76, 101,...      0   3\n",
       "4  [529, 76, 1509, 861, 1, 645, 1, 5013, 237, 3, ...      0   4"
      ]
     },
     "execution_count": 1622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load domain1 JSON as newline-delimited JSON (JSON Lines)\n",
    "df2 = pd.read_json('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/domain2_train_data.json', lines=True)\n",
    "print(\"Domain1 shape:\", df2.shape)\n",
    "print(\"Label distribution in domain1:\\n\", df2['label'].value_counts())\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa9977",
   "metadata": {},
   "source": [
    "#### Load the test dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "id": "46836131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape: (4000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[9159, 3048, 238, 276, 162, 286, 305, 22, 36, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[64, 5039, 1275, 6, 0, 871, 139, 270, 327, 237...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[327, 618, 76, 650, 121, 274, 1025, 0, 12207, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[6, 12, 609, 11905, 4, 879, 677, 78, 13352, 60...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 5504, 55, 22, 101, 3783, 139, 2664, 4, 1, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id\n",
       "0  [9159, 3048, 238, 276, 162, 286, 305, 22, 36, ...   0\n",
       "1  [64, 5039, 1275, 6, 0, 871, 139, 270, 327, 237...   1\n",
       "2  [327, 618, 76, 650, 121, 274, 1025, 0, 12207, ...   2\n",
       "3  [6, 12, 609, 11905, 4, 879, 677, 78, 13352, 60...   3\n",
       "4  [1, 5504, 55, 22, 101, 3783, 139, 2664, 4, 1, ...   4"
      ]
     },
     "execution_count": 1623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test JSON into a DataFrame and inspect\n",
    "df_test = pd.read_json('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/test_data.json', lines=True)\n",
    "print(\"Test set shape:\", df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbf302",
   "metadata": {},
   "source": [
    "#### Combine domain 1 and domina 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "id": "a15b9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  id   domain\n",
      "0  [6, 22, 34, 76, 501, 977, 1, 2514, 13623, 76, ...      0   0  domain1\n",
      "1  [222, 31, 4108, 104, 132, 361, 39, 2305, 12, 9...      0   1  domain1\n",
      "2  [736, 7194, 113, 12, 366, 2870, 123, 101, 12, ...      0   2  domain1\n",
      "3  [48, 1, 2025, 69, 361, 533, 327, 237, 4150, 13...      0   3  domain1\n",
      "4  [2973, 66, 1, 1493, 260, 2740, 50, 1027, 50, 1...      0   4  domain1\n",
      "Combined train shape: (6000, 4)\n",
      "\n",
      "Overall label distribution:\n",
      " label\n",
      "1    5250\n",
      "0     750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Domain breakdown:\n",
      " domain\n",
      "domain2    5000\n",
      "domain1    1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>[3293, 2487, 101, 12, 996, 69, 2487, 13, 206, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4962</td>\n",
       "      <td>domain2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1571</td>\n",
       "      <td>domain2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>[1, 26, 427, 47, 206, 421, 5989, 1, 2591, 39, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>domain2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>[1436, 4552, 101, 307, 48, 12, 316, 114, 635, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3480</td>\n",
       "      <td>domain2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>[6711, 238, 5410, 529, 222, 151, 12, 2971, 39,...</td>\n",
       "      <td>1</td>\n",
       "      <td>3602</td>\n",
       "      <td>domain2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label    id   domain\n",
       "5962  [3293, 2487, 101, 12, 996, 69, 2487, 13, 206, ...      1  4962  domain2\n",
       "2571  [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1...      1  1571  domain2\n",
       "1549  [1, 26, 427, 47, 206, 421, 5989, 1, 2591, 39, ...      1   549  domain2\n",
       "4480  [1436, 4552, 101, 307, 48, 12, 316, 114, 635, ...      1  3480  domain2\n",
       "4602  [6711, 238, 5410, 529, 222, 151, 12, 2971, 39,...      1  3602  domain2"
      ]
     },
     "execution_count": 1624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine domain1 and domain2 into a single training DataFrame and inspect\n",
    "train = pd.concat([\n",
    "    df1.assign(domain='domain1'), # add a new column 'domain' with value 'domain1'\n",
    "    df2.assign(domain='domain2') # in the new added column called 'domain', mark the value from domain 2 with value 'domain2'\n",
    "], ignore_index=True)\n",
    "\n",
    "print(train.head())\n",
    "\n",
    "print(\"Combined train shape:\", train.shape)\n",
    "print(\"\\nOverall label distribution:\\n\", train['label'].value_counts())\n",
    "print(\"\\nDomain breakdown:\\n\", train['domain'].value_counts())\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705bb5fc",
   "metadata": {},
   "source": [
    "## Method 1: Bag-of-Words + TF–IDF Baseline with Class-Weighted Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54108753",
   "metadata": {},
   "source": [
    "#### Prepare text strings for TF-IDF input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "id": "d50d278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>domain</th>\n",
       "      <th>text_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>domain1</td>\n",
       "      <td>6 22 34 76 501 977 1 2514 13623 76 31 2085 277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>domain1</td>\n",
       "      <td>222 31 4108 104 132 361 39 2305 12 936 1287 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>domain1</td>\n",
       "      <td>736 7194 113 12 366 2870 123 101 12 230 403 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>domain1</td>\n",
       "      <td>48 1 2025 69 361 533 327 237 4150 13 22 2128 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>domain1</td>\n",
       "      <td>2973 66 1 1493 260 2740 50 1027 50 1 3289 69 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label   domain                                           text_str\n",
       "0   0      0  domain1  6 22 34 76 501 977 1 2514 13623 76 31 2085 277...\n",
       "1   1      0  domain1  222 31 4108 104 132 361 39 2305 12 936 1287 66...\n",
       "2   2      0  domain1  736 7194 113 12 366 2870 123 101 12 230 403 51...\n",
       "3   3      0  domain1  48 1 2025 69 361 533 327 237 4150 13 22 2128 1...\n",
       "4   4      0  domain1  2973 66 1 1493 260 2740 50 1027 50 1 3289 69 5..."
      ]
     },
     "execution_count": 1625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert token sequences into strings\n",
    "train['text_str'] = train['text'].apply(lambda seq: ' '.join(map(str, seq)))\n",
    "df_test['text_str'] = df_test['text'].apply(lambda seq: ' '.join(map(str, seq)))\n",
    "\n",
    "train[['id', 'label', 'domain', 'text_str']].head() # inspect examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "id": "2273c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label    id   domain  \\\n",
      "384   [1, 231, 3252, 22, 595, 4, 133, 101, 4, 1496, ...      0   384  domain1   \n",
      "3366  [13739, 3355, 101, 1, 73, 13, 22, 238, 706, 49...      1  2366  domain2   \n",
      "1218  [76, 1509, 55, 305, 497, 12, 1326, 69, 635, 80...      0   218  domain2   \n",
      "1312  [324, 151, 285, 3050, 1106, 1, 5971, 453, 69, ...      1   312  domain2   \n",
      "873   [1882, 12, 1773, 507, 36, 64, 12, 630, 39, 168...      1   873  domain1   \n",
      "\n",
      "                                               text_str  \n",
      "384   1 231 3252 22 595 4 133 101 4 1496 4613 12 702...  \n",
      "3366  13739 3355 101 1 73 13 22 238 706 496 12 2547 ...  \n",
      "1218  76 1509 55 305 497 12 1326 69 635 80 1 279 76 ...  \n",
      "1312  324 151 285 3050 1106 1 5971 453 69 852 39 143...  \n",
      "873   1882 12 1773 507 36 64 12 630 39 1685 164 324 ...  \n",
      "\n",
      "\n",
      "384     domain1\n",
      "3366    domain2\n",
      "1218    domain2\n",
      "1312    domain2\n",
      "873     domain1\n",
      "         ...   \n",
      "2487    domain2\n",
      "2047    domain2\n",
      "3718    domain2\n",
      "3856    domain2\n",
      "4606    domain2\n",
      "Name: domain, Length: 4800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Split the combined DataFrame into df_train and df_val\n",
    "df_train, df_val = train_test_split(train, test_size=0.2, stratify = train['label'], random_state=42)\n",
    "\n",
    "print(df_train.head())\n",
    "print(\"\\n\")\n",
    "print(df_train.domain)\n",
    "\n",
    "# Extract labels\n",
    "y_train = df_train['label'].values\n",
    "y_val   = df_val['label'].values\n",
    "\n",
    "# Fit TF-IDF on the training text_str\n",
    "vectorizer = TfidfVectorizer(max_features=15000)\n",
    "X_train_tf = vectorizer.fit_transform(df_train['text_str'])\n",
    "X_val_tf   = vectorizer.transform(df_val['text_str'])\n",
    "X_test_tf  = vectorizer.transform(df_test['text_str'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cacf5d",
   "metadata": {},
   "source": [
    "#### Evulauate log-regression model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "id": "c0b17cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9250\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6546    0.8467    0.7384       150\n",
      "           1     0.9771    0.9362    0.9562      1050\n",
      "\n",
      "    accuracy                         0.9250      1200\n",
      "   macro avg     0.8159    0.8914    0.8473      1200\n",
      "weighted avg     0.9368    0.9250    0.9290      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate a class-weighted Logistic Regression baseline\n",
    "clf = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit on training TF-IDF features\n",
    "clf.fit(X_train_tf, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = clf.predict(X_val_tf)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71238f20",
   "metadata": {},
   "source": [
    "#### 5-Fold cross-validation of TF–IDF + logistic regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "id": "85ace36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracies per fold: [0.90583333 0.925      0.92       0.91333333 0.92916667]\n",
      "Mean CV accuracy: 0.9187 ± 0.0083\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=35000)),\n",
    "    ('clf',  LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Stratified 5-fold CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    pipeline,\n",
    "    train['text_str'],\n",
    "    train['label'],\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"CV accuracies per fold:\", scores)\n",
    "print(f\"Mean CV accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edb137",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: RandomizedSearchCV on TF–IDF + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "id": "dfb32943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "95 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.2466303785688064 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.4976995585890147 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.1654868492235873 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.1011181513932213 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.4543836610603706 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.1063105843862837 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.4348625053884851 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.070045328631458 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.0410832978860793 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.0060212415521588 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.0432878398284233 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.3567480913493766 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.4732030329772416 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.2201483081014035 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.2895297366043452 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 1.1250118818378816 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.93375    0.92       0.941875   0.93645833 0.92375    0.95020833\n",
      " 0.93541667        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.94875    0.96041667        nan\n",
      "        nan        nan        nan 0.88708333 0.928125          nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Accuracy: 0.9604\n",
      "Best Parameters: {'clf__C': 5.696204002320979, 'clf__penalty': None, 'tfidf__max_df': 0.6927494788845082, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 4)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # param_grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2), (1,3), (2,3), (3,3), (1,4)], # define the n-gram range\n",
    "#     'clf__C': [0.05, 0.1, 2, 3, 8],\n",
    "#     'clf__penalty': ['l1', 'l2', None], # l1 is for Lasso, l2 is for Ridge\n",
    "#     'tfidf__max_df': [0.5, 1.0], # max_df is the maximum document frequency\n",
    "#     'tfidf__min_df': [2, 5], # min_df is the minimum document frequency\n",
    "# }\n",
    "\n",
    "# param_grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3), (1, 4), (2, 4), (3, 4)],\n",
    "    'tfidf__max_df': uniform(loc=0.5, scale=1.0),  # samples from 0.5 to 1.5\n",
    "    'tfidf__min_df': [1, 2, 3, 5],\n",
    "    'clf__C': uniform(loc=0.01, scale=10),  # continuous range for C\n",
    "    'clf__penalty': ['l1', 'l2', None]\n",
    "} \n",
    "\n",
    "\n",
    "# # Perform grid search with GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator= pipeline,\n",
    "#     param_grid= param_grid,\n",
    "#     cv= cv,\n",
    "#     scoring= 'accuracy',\n",
    "#     n_jobs= -1,\n",
    "#     verbose= 1\n",
    "# )\n",
    "\n",
    "# Perform grid search with RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 30,\n",
    "    cv= cv,\n",
    "    scoring= 'accuracy',\n",
    "    n_jobs= -1,\n",
    "    verbose= 1\n",
    ")\n",
    "\n",
    "# # why use tarin['text_str'] and train['label]\n",
    "# grid_search.fit(train['text_str'], train['label']) \n",
    "\n",
    "# # Perform grid search with GridSearchCV\n",
    "# grid_search.fit(df_train['text_str'], df_train['label'])\n",
    "\n",
    "\n",
    "# Perform grid search with RandomizedSearchCV\n",
    "grid_search.fit(df_train['text_str'], df_train['label'])\n",
    "\n",
    "print(\"Best CV Accuracy: {:.4f}\".format(grid_search.best_score_))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d25cc",
   "metadata": {},
   "source": [
    "##### Re-evaluate model after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "id": "3736f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out Validation Accuracy: 0.9583333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9386    0.7133    0.8106       150\n",
      "           1     0.9604    0.9933    0.9766      1050\n",
      "\n",
      "    accuracy                         0.9583      1200\n",
      "   macro avg     0.9495    0.8533    0.8936      1200\n",
      "weighted avg     0.9577    0.9583    0.9558      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tuned model on hold-out validation set\n",
    "best_pipe = grid_search.best_estimator_\n",
    "X_val_tuned = best_pipe.named_steps['tfidf'].transform(df_val['text_str'])\n",
    "y_val_pred = best_pipe.named_steps['clf'].predict(X_val_tuned)\n",
    "\n",
    "print(\"Hold-out Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af2000",
   "metadata": {},
   "source": [
    "#### Threshold tuning for tuned TF–IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "id": "8c258d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold | Precision (human) | Recall (human) | F1 (human)\n",
      "  0.10    |   0.9328         |   0.7400     |  0.8253\n",
      "  0.15    |   0.9316         |   0.7267     |  0.8165\n",
      "  0.20    |   0.9310         |   0.7200     |  0.8120\n",
      "  0.25    |   0.9310         |   0.7200     |  0.8120\n",
      "  0.30    |   0.9391         |   0.7200     |  0.8151\n",
      "  0.35    |   0.9391         |   0.7200     |  0.8151\n",
      "  0.40    |   0.9391         |   0.7200     |  0.8151\n",
      "  0.45    |   0.9391         |   0.7200     |  0.8151\n",
      "  0.50    |   0.9386         |   0.7133     |  0.8106\n",
      "  0.55    |   0.9386         |   0.7133     |  0.8106\n",
      "  0.60    |   0.9381         |   0.7067     |  0.8061\n",
      "  0.65    |   0.9375         |   0.7000     |  0.8015\n",
      "  0.70    |   0.9375         |   0.7000     |  0.8015\n",
      "  0.75    |   0.9369         |   0.6933     |  0.7969\n",
      "  0.80    |   0.9369         |   0.6933     |  0.7969\n",
      "  0.85    |   0.9369         |   0.6933     |  0.7969\n",
      "  0.90    |   0.9364         |   0.6867     |  0.7923\n"
     ]
    }
   ],
   "source": [
    "best_pipe = grid_search.best_estimator_\n",
    "\n",
    "prob_human = best_pipe.predict_proba(df_val['text_str'])[:, 0]\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "print(\"Threshold | Precision (human) | Recall (human) | F1 (human)\")\n",
    "for t in thresholds:\n",
    "    preds = np.where(prob_human >= t, 0, 1)\n",
    "    # there is a higher chance of beinf a human (0) if prob_human >= t\n",
    "    prec = precision_score(y_val, preds, pos_label=0)\n",
    "    rec  = recall_score(y_val, preds, pos_label=0)\n",
    "    f1   = f1_score(y_val, preds, pos_label=0)\n",
    "    print(f\"  {t:.2f}    |   {prec:.4f}         |   {rec:.4f}     |  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "id": "c5f21fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_ngrom: (1, 4)\n",
      "Best_C: 5.696204002320979\n",
      "Best_clf_penalty: None\n",
      "[0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.86494265e-06 0.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 9.99999909e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "Validation Accuracy (threshold = 0.65): 0.9700\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9130    0.8400    0.8750       150\n",
      "           1     0.9774    0.9886    0.9830      1050\n",
      "\n",
      "    accuracy                         0.9700      1200\n",
      "   macro avg     0.9452    0.9143    0.9290      1200\n",
      "weighted avg     0.9694    0.9700    0.9695      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_ngram = grid_search.best_params_['tfidf__ngram_range']\n",
    "print(f\"Best_ngrom: {best_ngram}\")\n",
    "\n",
    "best_C = grid_search.best_params_['clf__C']\n",
    "print(f\"Best_C: {best_C}\")\n",
    "\n",
    "best_clf_penalty = grid_search.best_params_['clf__penalty']\n",
    "print(f\"Best_clf_penalty: {best_clf_penalty}\")\n",
    "\n",
    "\n",
    "# Train a new pipeline with the best parameters\n",
    "pipeline_tuned = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features = 30000, ngram_range = best_ngram)),\n",
    "    ('clf',  LogisticRegression(class_weight='balanced', C=best_C, penalty=best_clf_penalty, max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the tuned pipeline on the training set\n",
    "pipeline_tuned.fit(df_train['text_str'], df_train['label'])\n",
    "\n",
    "# Predict on the validation set\n",
    "prob_human = pipeline_tuned.predict_proba(df_val['text_str'])[:, 0]\n",
    "\n",
    "print(prob_human[:55])\n",
    "\n",
    "threshold = 0.65\n",
    "preds_val = np.where(prob_human >= threshold, 0, 1)\n",
    "print(preds_val[:55])\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Validation Accuracy (threshold = {threshold:.2f}): {accuracy_score(y_val, preds_val):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, preds_val, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f9a25",
   "metadata": {},
   "source": [
    "#### Predict y based on the test file data and export as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "id": "82bd5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_human_test = pipeline_tuned.predict_proba(df_test['text_str'])[:, 0]\n",
    "# threshold_final = threshold\n",
    "# preds_test = np.where(prob_human_test >= threshold, 0, 1)\n",
    "\n",
    "# submission = pd.DataFrame({'id': df_test['id'], 'class': preds_test})\n",
    "# submission.to_csv('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/tfidf2.csv', index=False)\n",
    "# submission.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fbed7",
   "metadata": {},
   "source": [
    "## Method 2: SMOTE on Domain 2’s Human Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "id": "d7d9aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import vstack, csr_matrix\n",
    "# from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "id": "b6137ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract TF–IDF vectorizer and optimal C from the tuned pipeline\n",
    "# vectorizer = best_pipe.named_steps['tfidf']\n",
    "# C_opt = best_pipe.named_steps['clf'].C\n",
    "\n",
    "# # Transform train/validation into feature matrices\n",
    "# X_train_all = vectorizer.transform(df_train['text_str'])\n",
    "# X_val_tf    = vectorizer.transform(df_val['text_str'])\n",
    "# y_train_all = df_train['label'].values\n",
    "\n",
    "# # Select Domain 2 subset\n",
    "# mask_dom2 = df_train['domain'] == 'domain2'\n",
    "# X_dom2    = X_train_all[mask_dom2].toarray()\n",
    "# y_dom2    = y_train_all[mask_dom2]\n",
    "\n",
    "# # Apply SMOTE on Domain 2 human class (label 0)\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_dom2_res, y_dom2_res = smote.fit_resample(X_dom2, y_dom2)\n",
    "\n",
    "# # Re-combine with Domain 1 unchanged\n",
    "# mask_dom1    = ~mask_dom2\n",
    "# X_dom1       = X_train_all[mask_dom1]\n",
    "# y_dom1       = y_train_all[mask_dom1]\n",
    "# X_dom2_res_sp = csr_matrix(X_dom2_res)\n",
    "\n",
    "# X_res = vstack([X_dom1, X_dom2_res_sp])\n",
    "# y_res = np.concatenate([y_dom1, y_dom2_res])\n",
    "\n",
    "# # Retrain Logistic Regression on SMOTE-augmented data\n",
    "# clf_smote = LogisticRegression(class_weight='balanced',\n",
    "#                                C=C_opt,\n",
    "#                                max_iter=1000,\n",
    "#                                random_state=42)\n",
    "# clf_smote.fit(X_res, y_res)\n",
    "\n",
    "# # Predict on validation using threshold = 0.70\n",
    "# prob_hum_smote = clf_smote.predict_proba(X_val_tf)[:, 0]\n",
    "# preds_smote    = np.where(prob_hum_smote >= 0.70, 0, 1)\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"SMOTE (Domain2 Human) Validation Accuracy:\", accuracy_score(y_val, preds_smote))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_val, preds_smote, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32813fb9",
   "metadata": {},
   "source": [
    "##### Threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "id": "e0cc642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_tf = vectorizer.transform(df_val['text_str'])\n",
    "\n",
    "# # Get human-written probabilities from your SMOTE-trained classifier\n",
    "# prob_human_smote = clf_smote.predict_proba(X_val_tf)[:, 0]\n",
    "\n",
    "# # Sweep thresholds and report metrics for the human class\n",
    "# thresholds = np.linspace(0.1, 0.9, 17)\n",
    "# print(\"Threshold | Precision(0) | Recall(0) | F1(0)\")\n",
    "# for t in thresholds:\n",
    "#     preds = np.where(prob_human_smote >= t, 0, 1)\n",
    "#     prec  = precision_score(y_val, preds, pos_label=0)\n",
    "#     rec   = recall_score(y_val, preds, pos_label=0)\n",
    "#     f1    = f1_score(y_val, preds, pos_label=0)\n",
    "#     print(f\"  {t:.2f}    |  {prec:.4f}   |   {rec:.4f}  |  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1637,
   "id": "a4ee4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.35\n",
    "# preds = np.where(prob_human_smote >= threshold, 0, 1)\n",
    "\n",
    "# print(f\"SMOTE Model Accuracy (threshold = {threshold:.2f}):\",\n",
    "#       accuracy_score(y_val, preds))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_val, preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7766841",
   "metadata": {},
   "source": [
    "#### Predict y based on the test file data and export as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1638,
   "id": "00af8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tf  = vectorizer.transform(df_test['text_str'])\n",
    "\n",
    "# prob_human_smote = clf_smote.predict_proba(X_test_tf)[:, 0]\n",
    "\n",
    "# preds_test = np.where(prob_human_smote >= 0.35, 0, 1)\n",
    "\n",
    "# submission_smote = pd.DataFrame({\n",
    "#     'id':    df_test['id'],\n",
    "#     'class': preds_test\n",
    "# })\n",
    "# submission_smote.to_csv('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/smote_submission_tfidf2.csv', index=False)\n",
    "# submission_smote.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d23cc",
   "metadata": {},
   "source": [
    "## Method 3: Domain-Expert Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd373e",
   "metadata": {},
   "source": [
    "#### Train domain-expert classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "id": "f3b074cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain‐expert models trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Subset by domain\n",
    "train_dom1 = df_train[df_train.domain == 'domain1']\n",
    "train_dom2 = df_train[df_train.domain == 'domain2']\n",
    "\n",
    "\n",
    "# Define identical function for each domain\n",
    "pipe_dom1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features = 24855, ngram_range = best_ngram)),\n",
    "    ('clf',  LogisticRegression(class_weight='balanced', C = best_C, penalty=best_clf_penalty, max_iter= 1120, random_state=42))\n",
    "])\n",
    "\n",
    "pipe_dom2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features = 24855, ngram_range = best_ngram)),\n",
    "    ('clf',  LogisticRegression(class_weight='balanced', C = best_C, penalty=best_clf_penalty, max_iter= 1120, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Train\n",
    "pipe_dom1.fit(train_dom1.text_str, train_dom1.label)\n",
    "pipe_dom2.fit(train_dom2.text_str, train_dom2.label)\n",
    "\n",
    "print(\"Domain‐expert models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59a376",
   "metadata": {},
   "source": [
    "##### Evaluate domain-expert ensemble on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1640,
   "id": "a3de3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy (thr=0.30): 0.9825\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9708    0.8867    0.9268       150\n",
      "           1     0.9840    0.9962    0.9901      1050\n",
      "\n",
      "    accuracy                         0.9825      1200\n",
      "   macro avg     0.9774    0.9414    0.9584      1200\n",
      "weighted avg     0.9824    0.9825    0.9822      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate an array for human-class probabilities\n",
    "probs = np.zeros(len(df_val))\n",
    "\n",
    "# Fill in per-domain probabilities\n",
    "mask_dom1 = df_val['domain'] == 'domain1'\n",
    "mask_dom2 = ~mask_dom1\n",
    "\n",
    "probs[mask_dom1] = pipe_dom1.predict_proba(df_val.loc[mask_dom1, 'text_str'])[:, 0]\n",
    "probs[mask_dom2] = pipe_dom2.predict_proba(df_val.loc[mask_dom2, 'text_str'])[:, 0]\n",
    "\n",
    "threshold = 0.30\n",
    "preds_ensemble = np.where(probs >= threshold, 0, 1)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Ensemble Validation Accuracy (thr={threshold:.2f}):\",\n",
    "      accuracy_score(y_val, preds_ensemble))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, preds_ensemble, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbc1d0",
   "metadata": {},
   "source": [
    "#### Threshold sweep for domain-expert ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "id": "5ed6ffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold | Precision(0) | Recall(0) | F1(0)\n",
      "  0.10    |   0.9433     |   0.8867   |  0.9141\n",
      "  0.15    |   0.9568     |   0.8867   |  0.9204\n",
      "  0.20    |   0.9638     |   0.8867   |  0.9236\n",
      "  0.25    |   0.9638     |   0.8867   |  0.9236\n",
      "  0.30    |   0.9708     |   0.8867   |  0.9268\n",
      "  0.35    |   0.9708     |   0.8867   |  0.9268\n",
      "  0.40    |   0.9851     |   0.8800   |  0.9296\n",
      "  0.45    |   0.9848     |   0.8667   |  0.9220\n",
      "  0.50    |   0.9847     |   0.8600   |  0.9181\n",
      "  0.55    |   0.9847     |   0.8600   |  0.9181\n",
      "  0.60    |   0.9923     |   0.8600   |  0.9214\n",
      "  0.65    |   0.9922     |   0.8533   |  0.9176\n",
      "  0.70    |   0.9922     |   0.8533   |  0.9176\n",
      "  0.75    |   0.9922     |   0.8467   |  0.9137\n",
      "  0.80    |   0.9922     |   0.8467   |  0.9137\n",
      "  0.85    |   0.9921     |   0.8333   |  0.9058\n",
      "  0.90    |   1.0000     |   0.8267   |  0.9051\n"
     ]
    }
   ],
   "source": [
    "# Compute ensemble “human” probabilities\n",
    "probs = np.zeros(len(df_val))\n",
    "mask_dom1     = df_val['domain'] == 'domain1'\n",
    "mask_dom2     = ~mask_dom1\n",
    "probs[mask_dom1] = pipe_dom1.predict_proba(df_val.loc[mask_dom1, 'text_str'])[:, 0]\n",
    "probs[mask_dom2] = pipe_dom2.predict_proba(df_val.loc[mask_dom2, 'text_str'])[:, 0]\n",
    "\n",
    "# Sweep thresholds\n",
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "print(\"Threshold | Precision(0) | Recall(0) | F1(0)\")\n",
    "for t in thresholds:\n",
    "    preds = np.where(probs >= t, 0, 1)\n",
    "    prec  = precision_score(y_val, preds, pos_label=0)\n",
    "    rec   = recall_score(   y_val, preds, pos_label=0)\n",
    "    f1    = f1_score(       y_val, preds, pos_label=0)\n",
    "    print(f\"  {t:.2f}    |   {prec:.4f}     |   {rec:.4f}   |  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331e6c6",
   "metadata": {},
   "source": [
    "#### Evaluate ensemble based attempts of different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1642,
   "id": "71423e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4929    False\n",
      "958      True\n",
      "1243    False\n",
      "1766    False\n",
      "2509    False\n",
      "        ...  \n",
      "5071    False\n",
      "2746    False\n",
      "5895    False\n",
      "2826    False\n",
      "1669    False\n",
      "Name: domain, Length: 1200, dtype: bool\n",
      "Ensemble Validation Accuracy (threshold = 0.30): 0.9825\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9708    0.8867    0.9268       150\n",
      "           1     0.9840    0.9962    0.9901      1050\n",
      "\n",
      "    accuracy                         0.9825      1200\n",
      "   macro avg     0.9774    0.9414    0.9584      1200\n",
      "weighted avg     0.9824    0.9825    0.9822      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = np.zeros(len(df_val)) \n",
    "mask_dom1 = df_val['domain'] == 'domain1'\n",
    "print(mask_dom1)\n",
    "mask_dom2 = ~mask_dom1\n",
    "# probs that the data from domain 1 is human-written\n",
    "probs[mask_dom1] = pipe_dom1.predict_proba(df_val.loc[mask_dom1, 'text_str'])[:, 0]\n",
    "# probs that the data from domain 2 is human-written\n",
    "probs[mask_dom2] = pipe_dom2.predict_proba(df_val.loc[mask_dom2, 'text_str'])[:, 0]\n",
    "\n",
    "# 2. Apply threshold\n",
    "# when the threshold is too high such as 0.75 and 0.80, even though the precision might be high like 98%\\\n",
    "# but the accuracy will be low, because of the overfitting. Therfore, the threshold should be set to 0.40 or 0.35\n",
    "threshold = 0.30\n",
    "preds_ensem = np.where(probs >= threshold, 0, 1)\n",
    "\n",
    "\n",
    "# 3. Evaluate\n",
    "print(f\"Ensemble Validation Accuracy (threshold = {threshold:.2f}):\",\n",
    "      accuracy_score(y_val, preds_ensem))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, preds_ensem, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a177aa2",
   "metadata": {},
   "source": [
    "#### Check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1643,
   "id": "ab056a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Training Accuracy (thr=0.30): 0.9997916666666666\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9983    1.0000    0.9992       600\n",
      "           1     1.0000    0.9998    0.9999      4200\n",
      "\n",
      "    accuracy                         0.9998      4800\n",
      "   macro avg     0.9992    0.9999    0.9995      4800\n",
      "weighted avg     0.9998    0.9998    0.9998      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute “human” probabilities on the training split\n",
    "probs_train = np.zeros(len(df_train))\n",
    "mask_tr_dom1 = df_train['domain'] == 'domain1'\n",
    "mask_tr_dom2 = ~mask_tr_dom1\n",
    "\n",
    "probs_train[mask_tr_dom1] = pipe_dom1.predict_proba(df_train.loc[mask_tr_dom1, 'text_str'])[:, 0]\n",
    "probs_train[mask_tr_dom2] = pipe_dom2.predict_proba(df_train.loc[mask_tr_dom2, 'text_str'])[:, 0]\n",
    "\n",
    "threshold = 0.30\n",
    "preds_train = np.where(probs_train >= threshold, 0, 1)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Ensemble Training Accuracy (thr={threshold:.2f}):\",\n",
    "      accuracy_score(df_train['label'], preds_train))\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(df_train['label'], preds_train, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1644,
   "id": "4061ff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  class\n",
       "0    0      0\n",
       "1    1      0\n",
       "2    2      0\n",
       "3    3      1\n",
       "4    4      0\n",
       "5    5      1\n",
       "6    6      0\n",
       "7    7      1\n",
       "8    8      0\n",
       "9    9      1\n",
       "10  10      1\n",
       "11  11      0\n",
       "12  12      0\n",
       "13  13      0\n",
       "14  14      0"
      ]
     },
     "execution_count": 1644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load & prepare test\n",
    "df_test = pd.read_json('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/test_data.json', lines=True)\n",
    "df_test['text_str'] = df_test['text'].apply(lambda seq: ' '.join(map(str, seq)))\n",
    "\n",
    "# Get human‐class probs from both domain‐expert models\n",
    "p1 = pipe_dom1.predict_proba(df_test['text_str'])[:,0]\n",
    "p2 = pipe_dom2.predict_proba(df_test['text_str'])[:,0]\n",
    "\n",
    "probs_test = (p1 + p2) / 2\n",
    "\n",
    "threshold_final = threshold\n",
    "preds_test = np.where(probs_test >= threshold, 0, 1)\n",
    "\n",
    "submission = pd.DataFrame({'id': df_test['id'], 'class': preds_test})\n",
    "submission.to_csv('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/ensemble_submission_tfidf2.csv', index=False)\n",
    "submission.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03f608",
   "metadata": {},
   "source": [
    "## Method of Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1645,
   "id": "d03fcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110ecc7",
   "metadata": {},
   "source": [
    "#### Load domain 1 data and split the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "id": "73d4b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain 1 shappe: (1000, 3)\n",
      "domain 1 number of label 0 and 1: 500, 500\n"
     ]
    }
   ],
   "source": [
    "# read the json file for domain 1\n",
    "d1_data = []\n",
    "with open('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        d1_data.append(json.loads(line))\n",
    "# convert the data to datareame and show the first 5 records\n",
    "d1_df = pd.DataFrame(d1_data)\n",
    "print(f\"domain 1 shappe: {d1_df.shape}\")\n",
    "\n",
    "d1_x, d1_y = d1_df['text'], d1_df['label']\n",
    "d1_x_training, d1_x_testing, d1_y_training, d1_y_testing = train_test_split(d1_x, d1_y, test_size=0.2, random_state=24)\n",
    "\n",
    "# show the number of label 0 and 1 overall\n",
    "print(f\"domain 1 number of label 0 and 1: {d1_y.value_counts()[0]}, {d1_y.value_counts()[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c75ec",
   "metadata": {},
   "source": [
    "#### Load domain 2 data and split the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1647,
   "id": "2c7e7a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain 2 shappe: (5000, 3)\n",
      "domain 2 number of label 0 and 1: 250, 4750\n"
     ]
    }
   ],
   "source": [
    "# read the json file for domain 2\n",
    "d2_data = []\n",
    "with open('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        d2_data.append(json.loads(line))\n",
    "# convert the data to datareame and show the first 5 records\n",
    "d2_df = pd.DataFrame(d2_data)\n",
    "print(f\"domain 2 shappe: {d2_df.shape}\")\n",
    "\n",
    "d2_x, d2_y = d2_df['text'], d2_df['label']\n",
    "d2_x_training, d2_x_testing, d2_y_training, d2_y_testing = train_test_split(d2_x, d2_y, test_size=0.2, random_state=24)\n",
    "\n",
    "# show the number of label 0 and 1 overall\n",
    "print(f\"domain 2 number of label 0 and 1: {d2_y.value_counts()[0]}, {d2_y.value_counts()[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02e3d4",
   "metadata": {},
   "source": [
    "#### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "id": "57a650ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shappe: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "# load test data \n",
    "test_data = []\n",
    "with open('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/test_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "# convert the data to datareame and show the first 5 records\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(f\"test data shappe: {test_df.shape}\")\n",
    "test_df.head()\n",
    "\n",
    "test_texts = []\n",
    "test_ids = []\n",
    "\n",
    "# convert the test data to string\n",
    "for text in test_df['text']:\n",
    "    str_text = str(text)\n",
    "    transfomed_str_text = str_text.replace(\",\", \" \").replace(\"]\", \"\").replace(\"[\", \"\")\n",
    "    test_texts.append(transfomed_str_text)\n",
    "\n",
    "for id in test_df['id']:\n",
    "    test_ids.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6023933",
   "metadata": {},
   "source": [
    "#### Vectorise the X (text) of traning and testing sets from domain 1 and 2, as well as the X (text) from test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1649,
   "id": "7f4855b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x_training = pd.concat([d1_x_training, d2_x_training], ignore_index=True)\n",
    "total_y_training = pd.concat([d1_y_training, d2_y_training], ignore_index=True)\n",
    "\n",
    "#  Apply CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Now the total_X_converted is a list of list of int,\\\n",
    "# converted it to as list of string in order to apply the CountVectorizer\n",
    "total_x_training_str = []\n",
    "for text in total_x_training:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    total_x_training_str.append(text_str)\n",
    "\n",
    "total_x_training_str_vec = vectorizer.fit_transform(total_x_training_str)\n",
    "\n",
    "\n",
    "# vectorize the training data from domain 1  \n",
    "d1_x_training_str = []\n",
    "for text in d1_x_training:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    d1_x_training_str.append(text_str)\n",
    "\n",
    "d1_x_training_str_vec = vectorizer.transform(d1_x_training_str)\n",
    "\n",
    "\n",
    "# vectorize the training data from domain 2\n",
    "d2_x_training_str = []\n",
    "for text in d2_x_training:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    d2_x_training_str.append(text_str)\n",
    "\n",
    "d2_x_training_str_vec = vectorizer.transform(d2_x_training_str)\n",
    "\n",
    "\n",
    "# vectorize the testing data from domain 1 \n",
    "d1_x_testing_str = []\n",
    "for text in d1_x_testing:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    d1_x_testing_str.append(text_str)\n",
    "\n",
    "d1_x_testing_str_vec = vectorizer.transform(d1_x_testing_str)\n",
    "\n",
    "\n",
    "# vectorize the testing data from domain 2\n",
    "d2_x_testing_str = []\n",
    "for text in d2_x_testing:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    d2_x_testing_str.append(text_str)\n",
    "\n",
    "d2_x_testing_str_vec = vectorizer.transform(d2_x_testing_str)\n",
    "\n",
    "# vectorize the data from test file\n",
    "test_texts_vec = []\n",
    "for text in test_texts:\n",
    "    vec_text = vectorizer.transform([text])\n",
    "    test_texts_vec.append(vec_text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b59da",
   "metadata": {},
   "source": [
    "#### Address and solve the class imbalance problem suing SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1650,
   "id": "afa05691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the issue of imbalanced data by applying SMOTE to domain 2\n",
    "smote = SMOTE(random_state=24)\n",
    "d2_x_training_smote, d2_y_training_smote = smote.fit_resample(d2_x_training_str_vec, d2_y_training)\n",
    "\n",
    "# create marks as 1 if the data comes domain 1 and 2 if the data comes forom domain 2\n",
    "y_total = [1] * len(d1_x_training) + [2] * len(d2_x_training)\n",
    "x_total = pd.concat([d1_x_training, d2_x_training], ignore_index=True)\n",
    "\n",
    "# fit the vectorizer to x_total before transforming\n",
    "x_total_str = []\n",
    "for text in x_total:\n",
    "    text_str = ' '.join(map(str,text))\n",
    "    x_total_str.append(text_str)\n",
    "    \n",
    "# vectorize the X training data from domain 1 and domain 2\n",
    "x_total_transformed = vectorizer.transform(x_total_str)\n",
    "\n",
    "# fix the issue of imbalanced data after domain\n",
    "smote_domain = SMOTE(random_state=24)\n",
    "x_total_transformed_smote, y_total_smote = smote_domain.fit_resample(x_total_transformed, y_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9845f4",
   "metadata": {},
   "source": [
    "#### Hyperparemater tuning for Random Forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1651,
   "id": "254c2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "6 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.88500277 0.91376381 0.88625121 0.90250439 0.8700215  0.86500896\n",
      "        nan        nan 0.88500746 0.90748876]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fitting parameters for domain 1: {'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "3 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.96618537 0.96697578 0.96631759 0.97013341 0.96802798 0.96644934\n",
      " 0.96144879 0.96710727        nan 0.96868622]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fitting parameters for domain 2: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "3 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.931378          nan 0.92500295 0.92362934 0.92837739 0.92812784\n",
      " 0.9160029  0.91700315 0.92625467 0.91987784]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fitting parameters for the domain classifier: {'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 6, 'n_estimators': 123}\n"
     ]
    }
   ],
   "source": [
    "# define paramter grids for GridSearchCV\n",
    "para_grid_d1 = {\n",
    "    'n_estimators': randint(50, 201), # more tree, more generalization capacity\n",
    "    'max_depth': randint(5,21), # higher values have more overfitting risk\n",
    "    'min_samples_split': randint(1, 11), # higher values reduce overfitting risk \n",
    "    'min_samples_leaf': randint(1, 11), # higher values lead to more generalization capacity\n",
    "}\n",
    "\n",
    "para_grid_d2 = {\n",
    "    'n_estimators': randint(50, 151), # more tree, more generalization capacity\n",
    "    'max_depth': randint(5, 11), # higher values have more overfitting risk\n",
    "    'min_samples_split': randint(1, 16), # higher values reduce overfitting risk \n",
    "    'min_samples_leaf': randint(1, 16), # higher values lead to more generalization capacity\n",
    "}\n",
    "\n",
    "para_grid_domain = {\n",
    "    'n_estimators': randint(50, 201), # more tree, more generalization capacity\n",
    "    'max_depth': randint(1,21), # higher values have more overfitting risk\n",
    "    'min_samples_split': randint(1, 11), # higher values reduce overfitting risk \n",
    "    'min_samples_leaf': randint(1, 11), # higher values lead to more generalization capacity\n",
    "}\n",
    "\n",
    "# define the RandomForestClassifier for domain 1 for classifying 1 or 0 in domain 1\n",
    "rf_d1 = RandomForestClassifier(random_state=24)\n",
    "# Search best parameters for domain 1\n",
    "# gs_d1 = GridSearchCV(estimator = rf_d1, param_grid = para_grid_d1, cv = 5, n_jobs = -1)\n",
    "gs_d1 = RandomizedSearchCV(estimator = rf_d1, param_distributions = para_grid_d1, n_iter=10, cv=3, n_jobs=-1)\n",
    "gs_d1.fit(d1_x_training_str_vec, d1_y_training)\n",
    "d1_best_params = gs_d1.best_params_\n",
    "print(f\"The best fitting parameters for domain 1: {d1_best_params}\")\n",
    "\n",
    "\n",
    "# define the RandomForestClassifier for classifying 1 or 0 in domain 2\n",
    "rf_d2 = RandomForestClassifier(random_state=24)\n",
    "# Search best paramters for domain 2\n",
    "# gs_d2 = GridSearchCV(estimator = rf_d2, param_grid = para_grid_d2, cv = 5, n_jobs = -1)\n",
    "gs_d2 = RandomizedSearchCV(estimator = rf_d2, param_distributions = para_grid_d2, n_iter=10, cv=3, n_jobs=-1)\n",
    "gs_d2.fit(d2_x_training_smote, d2_y_training_smote)\n",
    "d2_best_params = gs_d2.best_params_\n",
    "print(f\"The best fitting parameters for domain 2: {d2_best_params}\")\n",
    "\n",
    "\n",
    "# define the RandomForestClassifier for classifying domain 1 or domain 2\n",
    "rf_domain = RandomForestClassifier(random_state=24)\n",
    "# Search best parameters for domain classification \n",
    "# gs_domain = GridSearchCV(estimator = rf_domain, param_grid = para_grid_domain, cv = 5, n_jobs = -1)\n",
    "gs_domain = RandomizedSearchCV(estimator = rf_domain, param_distributions = para_grid_domain, n_iter=10, cv=3, n_jobs=-1)\n",
    "domain_best_params = gs_domain.fit(x_total_transformed_smote, y_total_smote)\n",
    "print(f\"The best fitting parameters for the domain classifier: {domain_best_params.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a915f07",
   "metadata": {},
   "source": [
    "#### Evaluate the trained model performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1652,
   "id": "684b5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of domain 1 model: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86        90\n",
      "           1       0.90      0.86      0.88       110\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.87      0.88      0.87       200\n",
      "weighted avg       0.88      0.88      0.88       200\n",
      "\n",
      "The performance of domain 2 model: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.64      0.52        50\n",
      "           1       0.98      0.96      0.97       950\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.71      0.80      0.74      1000\n",
      "weighted avg       0.95      0.94      0.95      1000\n",
      "\n",
      "The performance of domain classifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.97      0.95      4000\n",
      "           2       0.97      0.93      0.95      4000\n",
      "\n",
      "    accuracy                           0.95      8000\n",
      "   macro avg       0.95      0.95      0.95      8000\n",
      "weighted avg       0.95      0.95      0.95      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model performance for domain 1\n",
    "print(\"The performance of domain 1 model: \\n\")\n",
    "d1_y_pred = gs_d1.predict(d1_x_testing_str_vec)\n",
    "evaluate_report_d1 = classification_report(d1_y_testing, d1_y_pred)\n",
    "print(evaluate_report_d1)\n",
    "\n",
    "print(\"The performance of domain 2 model: \\n\")\n",
    "# evaluate the trained model performance for domain 2\n",
    "d2_y_pred = gs_d2.predict(d2_x_testing_str_vec)\n",
    "evaluate_report_d2 = classification_report(d2_y_testing, d2_y_pred)\n",
    "print(evaluate_report_d2)\n",
    "\n",
    "print(\"The performance of domain classifier: \\n\")\n",
    "# evalue the the trained model performance for domain classification\n",
    "domain_y_pred = gs_domain.predict(x_total_transformed_smote)\n",
    "evaluate_report_domain = classification_report(y_total_smote, domain_y_pred)\n",
    "print(evaluate_report_domain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa3144",
   "metadata": {},
   "source": [
    "#### Use the trained model to predict the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1653,
   "id": "9ff54669",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = []\n",
    "for text in test_texts:\n",
    "    # run the domain classifier to distinguish the data from domain 1 or domain 2\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    preicted_domain =  gs_domain.predict(text_vec)[0]\n",
    "    # if data from domain 1\n",
    "    if preicted_domain == 1:\n",
    "        # run the domain1 classifier to dishguish 1 or 0 within domain 1\n",
    "        predicted_label_d1 = gs_d1.predict(text_vec)[0]\n",
    "        y_pred_final.append(predicted_label_d1)\n",
    "    # if data from domain 2\n",
    "    if preicted_domain == 2:\n",
    "        # run the domain2 classifier to dishguish 1 or 0 within domain 2\n",
    "        predicted_label_d2 = gs_d2.predict(text_vec)[0]\n",
    "        y_pred_final.append(predicted_label_d2)\n",
    "\n",
    "# convert each predicted y to integer\n",
    "y_predicted_int = []\n",
    "for i in y_pred_final:\n",
    "    if i == '0':\n",
    "        y_predicted_int.append(0)\n",
    "    else:\n",
    "        y_predicted_int.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095f44b",
   "metadata": {},
   "source": [
    "#### Export output as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1654,
   "id": "457b2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "length of y_predicted_int: 4000\n",
      "length of test_ids: 4000\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_final[:10])\n",
    "print(y_predicted_int[:10])\n",
    "print(f\"length of y_predicted_int: {len(y_predicted_int)}\")\n",
    "print(f\"length of test_ids: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "id": "c97a3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct the output file\n",
    "final_output = pd.DataFrame({'id': test_ids, 'label': y_pred_final})\n",
    "# export to csv file\n",
    "final_output.to_csv('/Users/zigeliang/Desktop/All/Data Science 2025S1/COMP90051/A2/comp-90051-2025-s-1-project-2/rf_output3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df13230",
   "metadata": {},
   "source": [
    "#### Accuracy rate is 0.7160"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4b350",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
